{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcee083-80e0-4a06-860a-de8351bfa268",
   "metadata": {},
   "source": [
    "# <center>Machine Learning Project</center>\n",
    "\n",
    "** **\n",
    "## <center>*06 - Predictive Model*</center>\n",
    "\n",
    "** **\n",
    "\n",
    "The members of the `team` are:\n",
    "- Ana Farinha - 20211514\n",
    "- Francisco Capontes - 20211692\n",
    "- Sofia Gomes - 20240848\n",
    "- Rui Louren√ßo - 2021639"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9fbb0-e5cd-4b8b-9b43-917aa2930313",
   "metadata": {},
   "source": [
    "\n",
    "| Model              | Best Parameters  | Feature Selection | Average Train Macro F1 Score | Average Validation Macro F1-Score |\n",
    "|--------------------|--------------------------------------|---|---------------|---------------|\n",
    "| CatBoostClassifier | `{\"iterations\": 1000, \"learning_rate\": 0.11, \"depth\": 6, \"l2_leaf_reg\": 5, bagging_temperature\": 0.4}`| `None` | `0.46` | `0.42` |\n",
    "| XGBoostClassifier  | `{\"n_estimators\": 200, \"learning_rate\": 0.2, \"max_depth\": 7, \"subsample\": 0.9, \"colsample_bytree\": 0.9, \"gamma\": 0.3}`| `None` | `0.65`|`0.42`|\n",
    "| Decision Trees     | `{\"min_samples_split\": 10, \"min_samples_leaf\": 4, \"max_depth\": 20, \"criterion\": \"entropy\"}`| `Essential Features` | `0.43`| `0.31`|\n",
    "| Naive Bayes        | `Default Parameters`| `Essential Features` | `0.25`| `0.24`|\n",
    "| StackEnsemble      | `CatBoost Config 1, XGBoost Config 1, Decision Trees, Default Parameters`     | `Essential Features` | `0.31`| `0.30`|\n",
    "| VotingEnsemble     | `CatBoost Config 2, XGBoost Config 2, Decision Trees, Default Parameters` | `None` | `0.65`| `0.41`|\n",
    "| XGBoostClassifier With kfold | `{\"n_estimators\": 200, \"learning_rate\": 0.2, \"max_depth\": 7, \"subsample\": 0.9, \"colsample_bytree\": 0.9, \"gamma\": 0.3}`| `None` | `0.75`|`0.45`|\n",
    "\n",
    "Some models have different parameters configurations depending of the used feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9092592-0607-479b-9c85-23f4eea4ddab",
   "metadata": {},
   "source": [
    "After many iterations of preprocessing, modeling and gridsearch we found that XGBoostClassifier was slightly more consistent than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d971ae-afd5-44af-bfb1-d68601957cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#make the split here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "from utils_feature_selection import check_performace\n",
    "from utils_dicts import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state=68+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21023784-0a5b-4f1d-b236-a26832a01f0a",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 1. Import Dataset </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78208177-053c-41bd-a7b1-bbef43db1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "train_df = pd.read_csv('preprocessed_data/train_data.csv', index_col=\"Claim Identifier\")\n",
    "test_df = pd.read_csv('./preprocessed_data/test_data.csv', index_col = 'Claim Identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c75069-661f-4b8d-9160-ccae48dfc7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Selection: essential_features, reduced_features or []\n",
    "feature_selection = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd661e-4eeb-4d84-a5c0-151ff7070d37",
   "metadata": {},
   "source": [
    "Define y as a target \"Claim Injury Type Encoded\" and X with all the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2232728-35e4-4e12-8ec3-30483b640b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop([\"Claim Injury Type Encoded\"], axis = 1)\n",
    "y = train_df[\"Claim Injury Type Encoded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f0ce6-5d2f-454a-aee1-dbfd85da9848",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 2. Model Training</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05dd071-d71a-4bee-9c24-1e712e00896a",
   "metadata": {},
   "source": [
    "Defining the configuration for the model and the class mapping for the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e040a1ea-e2fb-4b9e-a495-21f826772fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"learning_rate\": 0.2,\n",
    "    \"max_depth\": 7,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"gamma\": 0.3,\n",
    "    \"random_state\": random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84430fe0-83de-4730-8e57-e56cf1b44143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0:'1. CANCELLED', \n",
    "    1:'2. NON-COMP',\n",
    "    2:'3. MED ONLY', \n",
    "    3:'4. TEMPORARY',\n",
    "    4:'5. PPD SCH LOSS', \n",
    "    5:'6. PPD NSL', \n",
    "    6:'7. PTD', \n",
    "    7:'8. DEATH'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58646c0-fc43-4001-884e-282b69cb47a3",
   "metadata": {},
   "source": [
    "We decided that instead of using a XGBoost Classification Model with a train_test_split to train the final model, we could implemented a pipeline that creates the 6 (number of folds) versions of the same model trained in diferent segments of the training dataset using Stratified K-fold.\n",
    "\n",
    "For the pipeline we first split the data into training set and validation set, and create copies of the training set and test_df in order to correctly preprocess the data.\n",
    "\n",
    "The scalers and models from the pipeline were saved in order to be used on the GrantApp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f3f980-124a-4321-8c6b-2ca754dd6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=6, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15aeaff0-c520-4de4-a2c1-f2475506b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1...\n",
      "\n",
      "Fold 1 train F1 score: 0.7488\n",
      "Fold 1 validation F1 score: 0.4336\n",
      "------------------------------\n",
      "Processing Fold 2...\n",
      "\n",
      "Fold 2 train F1 score: 0.7505\n",
      "Fold 2 validation F1 score: 0.4446\n",
      "------------------------------\n",
      "Processing Fold 3...\n",
      "\n",
      "Fold 3 train F1 score: 0.7552\n",
      "Fold 3 validation F1 score: 0.4567\n",
      "------------------------------\n",
      "Processing Fold 4...\n",
      "\n",
      "Fold 4 train F1 score: 0.7537\n",
      "Fold 4 validation F1 score: 0.4405\n",
      "------------------------------\n",
      "Processing Fold 5...\n",
      "\n",
      "Fold 5 train F1 score: 0.7581\n",
      "Fold 5 validation F1 score: 0.4526\n",
      "------------------------------\n",
      "Processing Fold 6...\n",
      "\n",
      "Fold 6 train F1 score: 0.7549\n",
      "Fold 6 validation F1 score: 0.4467\n",
      "------------------------------\n",
      "Average Train F1 score: 0.7535329372930706\n",
      "Average Validation F1 score: 0.4457753473919755\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.zeros((len(test_df), len(class_mapping)))\n",
    "avg_train = []\n",
    "avg_val = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Processing Fold {fold + 1}...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    test_temp =test_df.copy()\n",
    "    train_temp=X_train.copy()\n",
    "    \n",
    "    # Preprocess X_train and X_Val\n",
    "    remove_outliers(X_train)\n",
    "    X_train, X_val = apply_frequency_encoding(X_train, X_val, True, fold=fold)\n",
    "    NA_imputer(X_train,X_val, True, fold=fold)\n",
    "    create_new_features(X_train,X_val)\n",
    "    \n",
    "    # Preprocess Test_df\n",
    "    remove_outliers(train_temp)\n",
    "    train_temp, test_temp = apply_frequency_encoding(train_temp, test_temp)\n",
    "    NA_imputer(train_temp, test_temp)\n",
    "    create_new_features(train_temp, test_temp)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train[numerical_features])\n",
    "    # Save Scaler\n",
    "    joblib.dump(scaler, f'./OthersPipeline/Scaler_{fold}.pkl')\n",
    "    X_train[numerical_features]  = scaler.transform(X_train[numerical_features])\n",
    "    X_val[numerical_features]  = scaler.transform(X_val[numerical_features])  \n",
    "    test_temp[numerical_features]  = scaler.transform(test_temp[numerical_features])  \n",
    "\n",
    "    drop_list = []\n",
    "    if feature_selection != []:\n",
    "        for col in X_train.columns:\n",
    "            if col not in feature_selection:\n",
    "                drop_list.append(col)\n",
    "\n",
    "    X_train = X_train.drop(drop_list, axis=1)\n",
    "    X_val = X_val.drop(drop_list, axis=1)\n",
    "    test_temp = test_temp.drop(drop_list, axis=1)\n",
    "        \n",
    "    # Train model\n",
    "    model = XGBClassifier(\n",
    "            n_estimators=config[\"n_estimators\"],        \n",
    "            learning_rate=config[\"learning_rate\"],      \n",
    "            max_depth=config[\"max_depth\"],                          \n",
    "            subsample=config[\"subsample\"],              \n",
    "            colsample_bytree=config[\"colsample_bytree\"],\n",
    "            gamma=config[\"gamma\"],                     \n",
    "            objective=\"multi:softmax\",                  \n",
    "            num_class=8,                                \n",
    "            eval_metric=\"merror\",   \n",
    "            random_state=config[\"random_state\"],                                      \n",
    "            verbosity=0                                 \n",
    "        )\n",
    "    model.fit(X_train,y_train)\n",
    "    # Save Model\n",
    "    model.save_model(f\"./OpenEnded/Model_{fold}.json\")\n",
    "    \n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_val = model.predict(X_val)\n",
    "    \n",
    "    f1_train = f1_score(y_train, pred_train, average='macro')\n",
    "    f1_val = f1_score(y_val, pred_val, average='macro')\n",
    "\n",
    "    avg_train.append(f1_train)\n",
    "    avg_val.append(f1_val)\n",
    "\n",
    "    print(f\"Fold {fold + 1} train F1 score: {f1_train:.4f}\")\n",
    "    print(f\"Fold {fold + 1} validation F1 score: {f1_val:.4f}\")\n",
    "    print(f\"------------------------------\")\n",
    "    \n",
    "    test_preds += model.predict_proba(test_temp)\n",
    "\n",
    "print(f\"Average Train F1 score: {sum(avg_train)/len(avg_train)}\")\n",
    "print(f\"Average Validation F1 score: {sum(avg_val)/len(avg_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3dbbe7-b56b-4cd0-a80f-8ccaa2fc1170",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 3. Test Predictions </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e110af5-76f3-4843-ac6c-9f1dba18cff7",
   "metadata": {},
   "source": [
    "To calculate the **final prediction** we first averaged the test predictions of each fold by the number of total folds and then used **np.argmax** to select the class with the highest averaged probability.\n",
    "The final step was to create a dataframe with the index of test_df as **Claim Identifier** and the **Claim Injury Type** as the inversed mapping of the **final prediction** and **classes_mapping**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f8f088-4797-44d6-a3d4-f654ef8e87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c0f48b-8031-4a2e-a948-a5177ff04a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_preds = np.argmax(test_preds / kf.get_n_splits(), axis=1)\n",
    "submission_df = pd.DataFrame({\n",
    "    'Claim Identifier': test_id,\n",
    "    'Claim Injury Type': final_test_preds\n",
    "})\n",
    "submission_df[\"Claim Injury Type\"] = submission_df[\"Claim Injury Type\"].replace(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "410e922e-c323-4bf9-81be-1606a606da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    version = version_control()\n",
    "    submission_df.to_csv(f'./submissions/Group49_Version{version:02}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
